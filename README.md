# LiBian Metrics / å¤æ–‡å­—è´¨é‡è¯„ä¼°å·¥å…·# LiBian Metrics



[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org)**LiBian Metrics** is a Python toolkit for quantitative assessment of ancient character (glyph) image quality. It provides six key metrics and a weighted composite score (LQI) to evaluate the visual characteristics of historical character forms.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Features

**LiBian Metrics** is a Python toolkit for quantitative quality assessment of ancient character (glyph) images. It provides 6 quantitative metrics and a weighted composite quality index (LQI) for analyzing character shape, stroke patterns, and spatial distribution.

### Six Quantitative Metrics

**LiBianæŒ‡æ ‡å·¥å…·**æ˜¯ç”¨äºå¤æ–‡å­—å­—å½¢å›¾ç‰‡é‡åŒ–è´¨é‡è¯„ä¼°çš„Pythonå·¥å…·åŒ…ã€‚å®ƒæä¾›6ä¸ªé‡åŒ–æŒ‡æ ‡å’Œä¸€ä¸ªåŠ æƒç»¼åˆåˆ†ï¼ˆLQIï¼‰ï¼Œç”¨äºåˆ†æå­—çš„ç»“ä½“å¸ƒå±€ã€ç¬”ç”»ç¬”åŠ¿å’Œç©ºé—´åˆ†å¸ƒã€‚

#### Layout Metrics (æ€»æƒé‡ 0.55)

## ğŸ“‹ Features / åŠŸèƒ½ç‰¹æ€§- **SSI (Shape Squareness Index)** [0.20]: Measures how square-shaped the outer contour is

- **GCP (Global Centering of Mass)** [0.10]: Evaluates proximity of foreground center of mass to bounding box center

### Supported Metrics / æ”¯æŒçš„æŒ‡æ ‡- **SSD (Spatial Sparsity-Dispersion)** [0.25]: Assesses uniformity of foreground distribution across space



#### Layout & Shape Metrics / ç»“ä½“å¸ƒå±€ç±» (Weight: 55%)#### Stroke Metrics (æ€»æƒé‡ 0.45)

- **SSI (Shape Squareness Index)** - å¤–éƒ¨è½®å»“æ–¹æ•´åº¦- **STR (Straightness Ratio)** [0.15]: Proportion of straight stroke segments detected via Hough lines

  - Measures how square/rectangular the character outline is- **CSI (Corner Sharpness Index)** [0.15]: Intensity of corner/junction sharpness in strokes

  - è¡¡é‡å­—çš„å¤–è½®å»“çš„æ–¹æ­£ç¨‹åº¦- **COI (Connectivity & Overlap Index)** [0.15]: Complexity of branching and loop structures



- **GCP (Global Centering of Mass)** - é‡å¿ƒå±…ä¸­åº¦### Composite Score

  - Evaluates how centered the character weight distribution is- **LQI (Libian Quality Index)**: Weighted average of all six metrics

  - è¯„ä¼°å­—çš„é‡å¿ƒä½ç½®çš„å±…ä¸­åº¦  ```

  LQI = 0.20Â·SSI + 0.10Â·GCP + 0.25Â·SSD + 0.15Â·STR + 0.15Â·CSI + 0.15Â·COI

- **SSD (Spatial Sparsity-Dispersion)** - ç©ºé—´ç–å¯†ç¦»æ•£åº¦  ```

  - Analyzes the balance of pixel distribution across the character

  - åˆ†æå­—å†…åƒç´ åˆ†å¸ƒçš„ç–å¯†å¹³è¡¡åº¦## Installation



#### Stroke & Trajectory Metrics / ç¬”ç”»ç¬”åŠ¿ç±» (Weight: 45%)### Prerequisites

- **STR (Straightness Ratio)** - ç›´çº¿åŒ–æ¯”ä¾‹- Python 3.10+

  - Measures the ratio of straight strokes vs curves- pip

  - è¡¡é‡ç¬”ç”»çš„ç›´çº¿åŒ–ç¨‹åº¦

### From Source

- **CSI (Corner Sharpness Index)** - æ–¹æŠ˜å°–é”åº¦

  - Evaluates corner sharpness and angular connections```bash

  - è¯„ä¼°è§’ç‚¹çš„å°–é”ç¨‹åº¦å’Œæ–¹æŠ˜ç‰¹å¾cd LiShift

pip install -e .

- **COI (Connectivity & Overlap Index)** - è¿æ¥/äº¤é‡å¤åˆæŒ‡æ•°```

  - Analyzes branching density and intersection patterns

  - åˆ†æç¬”ç”»åˆ†å‰å’Œäº¤é‡çš„ç‰¹å¾This installs the package in editable mode along with all dependencies:

- opencv-python

### Composite Score / ç»¼åˆåˆ†æ•°- numpy

- **LQI (Libian Quality Index)** - åŠ æƒç»¼åˆåˆ†- scikit-image

  - Weighted combination of all 6 metrics- scikit-learn

  - æ‰€æœ‰æŒ‡æ ‡çš„åŠ æƒç»¼åˆåˆ†- joblib

- pandas

## ğŸš€ Quick Start / å¿«é€Ÿå¼€å§‹

## Quick Start

### Installation / å®‰è£…

### Command Line Usage

```bash

cd /path/to/LiShift#### Single Image Analysis

pip install -e .```bash

```python -m libian_metrics --image path/to/glyph.jpg

```

Or install dependencies directly:

```bashOutput (JSON):

pip install opencv-python numpy scikit-image scikit-learn joblib pandas matplotlib```json

```{

  "image": "path/to/glyph.jpg",

### Usage Modes / ä½¿ç”¨æ¨¡å¼  "SSI": 0.71,

  "GCP": 0.88,

#### Mode 1: Single Image Processing / å•å›¾å¤„ç†  "SSD": 0.63,

  "STR": 0.76,

Process a single character image:  "CSI": 0.54,

```bash  "COI": 0.41,

python -m libian_metrics --image path/to/char.jpg  "LQI": 0.64,

```  "angle": -1.8,

  "bbox": [12, 245, 30, 238],

With calibration and output:  "scale": 0.78,

```bash  "quality_flag": true,

python -m libian_metrics --image char.jpg --calib calibration.json --out result.json  "skel_pixels": 1245,

```  "max_component_area": 8920

}

#### Mode 2: Batch Dataset Processing / æ‰¹é‡æ•°æ®é›†å¤„ç†```



Process an entire dataset folder with organized character subfolders:#### With Calibration

```bash

```bashpython -m libian_metrics --image glyph.jpg --calib calibration.json --out result.json

python -m libian_metrics --dataset data/my_dataset --out results/output.json```

```

#### Batch CSV Output

With calibration and detailed metrics:```bash

```bashpython -m libian_metrics --image glyph1.jpg --csv results.csv

python -m libian_metrics --dataset data/my_dataset --calib calibration.json --out results/output.json --detailedpython -m libian_metrics --image glyph2.jpg --csv results.csv

```# Each call appends to results.csv

```

## ğŸ“ Dataset Structure / æ•°æ®é›†ç»“æ„

#### All Options

For batch processing, organize your data as follows:```bash

```python -m libian_metrics --help

data/```

â””â”€â”€ dataset_name/

    â”œâ”€â”€ å­—1/```

    â”‚   â”œâ”€â”€ image1.jpgOptions:

    â”‚   â”œâ”€â”€ image2.jpg  --image PATH           Path to input image (JPG/PNG) [required]

    â”‚   â””â”€â”€ image3.png  --calib PATH           Path to calibration JSON file [optional]

    â”œâ”€â”€ å­—2/  --out PATH             Output JSON file path (default: stdout)

    â”‚   â”œâ”€â”€ image1.jpg  --csv PATH             Output CSV file path (appends if exists)

    â”‚   â””â”€â”€ image2.jpg  --debug                Enable debug output

    â”œâ”€â”€ å­—3/```

    â”‚   â””â”€â”€ image1.jpg

    â””â”€â”€ ...more characters...### Python API

```

```python

**Important notes:**from libian_metrics import preprocess, compute_all_metrics

- Each character has its own folder named with the character (e.g., "ç”²", "ä¹™", "ä¸™")from libian_metrics.io_utils import read_image

- Images in each folder will be processed and averaged to get the character's metricsimport json

- Supported formats: JPG, JPEG, PNG, BMP, TIFF

- Results will show average metrics Â± standard deviation for each character# Load and preprocess image

img = read_image('glyph.jpg')

**é‡è¦è¯´æ˜:**bin_img, skel, meta = preprocess(img, target_height=256)

- æ¯ä¸ªå­—çš„å›¾ç‰‡æ”¾åœ¨ä»¥è¯¥å­—å‘½åçš„æ–‡ä»¶å¤¹ä¸­ï¼ˆå¦‚"ç”²", "ä¹™", "ä¸™"ï¼‰

- ç³»ç»Ÿä¼šå¤„ç†æ¯ä¸ªå­—çš„æ‰€æœ‰å›¾ç‰‡ï¼Œå¹¶è®¡ç®—å¹³å‡æŒ‡æ ‡# Compute metrics

- æ”¯æŒæ ¼å¼ï¼šJPG, JPEG, PNG, BMP, TIFFmetrics = compute_all_metrics(bin_img, skel)

- ç»“æœæ˜¾ç¤ºæ¯ä¸ªå­—çš„å¹³å‡æŒ‡æ ‡ Â± æ ‡å‡†å·®

# With calibration

## ğŸ“Š Output Format / è¾“å‡ºæ ¼å¼import json

with open('calibration.json') as f:

### Batch Processing Output / æ‰¹é‡å¤„ç†è¾“å‡º    calib = json.load(f)

metrics = compute_all_metrics(bin_img, skel, calib)

**Single Character Result (å•å­—ç»“æœ):**

```jsonprint(f"LQI Score: {metrics['LQI']:.3f}")

{```

  "char": "ç”²",

  "SSI": 0.71,## Calibration

  "SSI_std": 0.05,

  "GCP": 0.88,Generate calibration parameters from a set of sample images:

  "GCP_std": 0.03,

  "SSD": 0.63,```bash

  "SSD_std": 0.08,python -c "

  "STR": 0.76,from libian_metrics.calibrate import calibrate_from_folder, save_calibration

  "STR_std": 0.06,calib = calibrate_from_folder('path/to/samples', sample_n=50)

  "CSI": 0.54,save_calibration(calib, 'calibration.json')

  "CSI_std": 0.07,"

  "COI": 0.41,```

  "COI_std": 0.09,

  "LQI": 0.64,Or use the calibration script:

  "LQI_std": 0.06,

  "image_count": 5```python

}from libian_metrics.calibrate import calibrate_from_folder, save_calibration

```

# Calibrate from sample images

**Batch Dataset Result (æ•°æ®é›†ç»“æœ):**calib = calibrate_from_folder(

```json    'samples/',

{    sample_n=100,

  "dataset_name": "my_dataset",    r_cap_percentile=99.0,

  "dataset_path": "/path/to/data/my_dataset",    c_cap_percentile=95.0

  "timestamp": "2024-01-06T23:30:00",)

  "characters": {

    "ç”²": {# Save calibration

      "SSI": 0.71,save_calibration(calib, 'calibration.json')

      "SSI_std": 0.05,```

      ...

      "LQI": 0.64,The calibration process:

      "image_count": 51. Loads all images from the specified folder

    },2. Extracts statistical distributions of key parameters

    "ä¹™": {3. Computes percentiles for normalization

      "SSI": 0.68,4. Generates `calibration.json` with optimized parameters

      ...

    }## Project Structure

  },

  "summary": {```

    "total_characters": 10,LiShift/

    "total_images": 45,â”œâ”€â”€ libian_metrics/

    "average_LQI": 0.65,â”‚   â”œâ”€â”€ __init__.py          # Package initialization

    "lqi_min": 0.52,â”‚   â”œâ”€â”€ __main__.py          # Module entry point

    "lqi_max": 0.78â”‚   â”œâ”€â”€ cli.py               # Command-line interface

  }â”‚   â”œâ”€â”€ io_utils.py          # I/O and utility functions

}â”‚   â”œâ”€â”€ preprocess.py        # Preprocessing pipeline

```â”‚   â”œâ”€â”€ skeleton.py          # Skeleton extraction

â”‚   â”œâ”€â”€ metrics.py           # Core metric computations

## ğŸ› ï¸ Advanced Usage / é«˜çº§ç”¨æ³•â”‚   â””â”€â”€ calibrate.py         # Calibration utilities

â”œâ”€â”€ tests/

### Command-line Options / å‘½ä»¤è¡Œé€‰é¡¹â”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ test_metrics.py      # Unit tests

```bashâ”‚   â”œâ”€â”€ generate_samples.py  # Sample image generator

python -m libian_metrics --helpâ”‚   â””â”€â”€ sample_images/       # Test images

```â”œâ”€â”€ runs/                    # Debug outputs (optional)

â”œâ”€â”€ setup.py

**Common options:**â”œâ”€â”€ pyproject.toml

- `--image PATH`: Input image file pathâ””â”€â”€ README.md

- `--dataset PATH`: Dataset folder with character subfolders```

- `--calib PATH`: Calibration JSON file for custom parameters

- `--out PATH`: Output JSON file path## Preprocessing Pipeline

- `--csv PATH`: Output CSV file path (single image mode)

- `--detailed`: Show detailed metrics table in consoleThe preprocessing module includes:

- `--visualize`: Generate visualization charts

- `--viz-dir PATH`: Directory to save visualizations (default: runs/)1. **Grayscale Conversion**: BGR â†’ Grayscale

- `--debug`: Enable debug output2. **Adaptive Binarization**: `cv2.adaptiveThreshold` (GAUSSIAN_C, blockSize=35)

3. **Component Filtering**: Remove small connected components (< 0.02% of image area)

### Creating Custom Calibration / åˆ›å»ºè‡ªå®šä¹‰æ ¡å‡†æ–‡ä»¶4. **Skew Correction**: Estimate and correct rotation angle (Â±5Â°)

5. **Rescaling**: Normalize to target height (default 256px)

Default calibration values are used if no calibration file is provided.6. **Skeletonization**: Extract skeleton using `skimage.morphology.skeletonize`

7. **Spur Pruning**: Remove short branches (< 6px)

To create a custom calibration file:8. **Quality Checks**: Flag images with insufficient content

```bash

python -c "## Metrics Details

import json

calib = {### SSI (Shape Squareness Index)

    'r_cap': 3.0,      # Shape squareness cap- Uses minimum area rotated rectangle from largest component

    'c_cap': 1.0,      # Sparsity cap- Aspect ratio: `r = max(w,h) / min(w,h)`

    # Add more parameters as needed- Formula: `SSI = 1 - |log(r)| / |log(r_cap)|`

}- Higher values indicate more square-shaped characters

with open('calibration.json', 'w') as f:

    json.dump(calib, f, indent=2)### GCP (Global Centering of Mass)

"- Compares center of mass with bounding box center

```- Formula: `GCP = 1 - distance / diagonal`

- Closer to 1.0 indicates better centering

## ğŸ“ˆ Processing Examples / å¤„ç†ç¤ºä¾‹

### SSD (Spatial Sparsity-Dispersion)

### Example 1: Process Single Image- Divides image into 5Ã—5 grid

```bash- Calculates coefficient of variation (CV) of pixel density

python -m libian_metrics --image sample.jpg --out sample_result.json- Formula: `SSD = 1 - min(CV, c_cap) / c_cap`

```- Higher values indicate more uniform distribution



### Example 2: Batch Process Oracle Bone Inscriptions### STR (Straightness Ratio)

```bash- Uses probabilistic Hough line detection on skeleton

# Assuming data/oracle_bones/ contains character subfolders- Filters by dominant angles (0Â°, 45Â°, 90Â°, 135Â°)

python -m libian_metrics --dataset data/oracle_bones --out results/oracle_results.json --detailed- Formula: `STR = straight_pixels / total_skeleton_pixels`

```- Higher values indicate more linear strokes



### Example 3: Batch Process with Visualization### CSI (Corner Sharpness Index)

```bash- Analyzes turning angles in skeleton paths

python -m libian_metrics \- Identifies sharp corners (angle changes > threshold)

  --dataset data/bronze_inscriptions \- Combines angle sharpness with corner density

  --calib calibration.json \- Higher values indicate sharper, more angular characters

  --out results/bronze_results.json \

  --visualize \### COI (Connectivity & Overlap Index)

  --viz-dir results/viz- Counts branching points (degree â‰¥ 3)

```- Estimates loops and overlapping regions

- Formula: `COI = Î±Â·branch_density + (1-Î±)Â·loop_density`

## ğŸ” Understanding the Metrics / æŒ‡æ ‡è§£é‡Š- Higher values indicate more complex connectivity



### SSI (0-1) / æ–¹æ•´åº¦## Quality Flags

- **1.0**: Perfect square/rectangle outline

- **0.5-0.8**: Normal characters with varied aspect ratioImages are marked with `quality_flag=false` if:

- **<0.5**: Very elongated or irregular outlines- Largest component area < 1% of image area

- Skeleton pixels < 200

### GCP (0-1) / é‡å¿ƒå±…ä¸­åº¦- Hough line coverage < 5% of skeleton (noise indicator)

- **1.0**: Weight perfectly centered

- **0.7-0.9**: Well-centered characterSuch images should be filtered out in downstream analysis.

- **<0.7**: Off-center weight distribution

## Testing

### SSD (0-1) / ç–å¯†ç¦»æ•£åº¦

- **1.0**: Perfect uniform pixel distribution### Generate Sample Images

- **0.6-0.8**: Well-balanced density```bash

- **<0.6**: Highly variable pixel distributioncd tests

python generate_samples.py

### STR (0-1) / ç›´çº¿åŒ–æ¯”ä¾‹```

- **1.0**: Mostly straight strokes

- **0.5-0.8**: Mix of straight and curved strokesThis creates three sample images:

- **<0.5**: Mostly curved strokes- `sample_1.png`: Well-formed square character

- `sample_2.png`: Character with regular strokes

### CSI (0-1) / æ–¹æŠ˜å°–é”åº¦- `sample_3.png`: Asymmetric character

- **1.0**: Very sharp corners

- **0.4-0.7**: Normal corner sharpness### Run Unit Tests

- **<0.4**: Rounded, smooth connections```bash

python -m pytest tests/ -v

### COI (0-1) / è¿æ¥äº¤é‡åº¦```

- **0.7-1.0**: High connectivity/overlap

- **0.3-0.7**: Medium connectivityOr:

- **<0.3**: Low connectivity```bash

python -m unittest discover tests/ -v

## ğŸ“ Configuration / é…ç½®è¯´æ˜```



### Default Parameters / é»˜è®¤å‚æ•°### End-to-End Test

```bash

| Parameter | Default | Description / è¯´æ˜ |# Single image

|-----------|---------|-------------|python -m libian_metrics --image tests/sample_images/sample_1.png

| `target_height` | 256 | Normalized height of character / å­—çš„æ ‡å‡†åŒ–é«˜åº¦ |

| `small_comp_ratio` | 2e-4 | Threshold for removing small components / å°è¿é€šåŸŸé˜ˆå€¼ |# Batch processing

| `grid_size` | 5 | Grid size for sparsity analysis / ç–å¯†åˆ†æç½‘æ ¼å¤§å° |for img in tests/sample_images/*.png; do

| `angle_thresh_deg` | 30 | Angle threshold for corner detection / è§’ç‚¹æ£€æµ‹è§’åº¦é˜ˆå€¼ |    python -m libian_metrics --image "$img" --csv results.csv

done

All parameters can be customized through the calibration file.

# View results

## ğŸ§ª Testing / æµ‹è¯•cat results.csv

```

To test with sample images:

```bash## Example Output

# Create sample dataset / åˆ›å»ºç¤ºä¾‹æ•°æ®é›†

mkdir -p data/test_chars/{ç”²,ä¹™,ä¸™}### JSON Format (Single Image)

```json

# Copy your test images / å¤åˆ¶æµ‹è¯•å›¾ç‰‡{

# cp your_images/*.jpg data/test_chars/ç”²/  "image": "tests/sample_images/sample_1.png",

  "SSI": 0.82,

# Process the dataset / å¤„ç†æ•°æ®é›†  "GCP": 0.91,

python -m libian_metrics --dataset data/test_chars --out results/test_results.json --detailed  "SSD": 0.74,

```  "STR": 0.68,

  "CSI": 0.59,

## âš ï¸ Quality Control Flags / è´¨é‡æ§åˆ¶  "COI": 0.45,

  "LQI": 0.71,

The system tracks a `quality_flag` for each image. This flag is set to `False` when:  "angle": 0.3,

- Maximum connected component area < 1% of image area  "bbox": [40, 216, 40, 216],

- Skeleton pixels < 200  "scale": 1.0,

- Hough line coverage < 5% of skeleton  "quality_flag": true,

  "skel_pixels": 425,

ç»“æœä¸­çš„ `quality_flag` ä¸º `False` è¡¨ç¤ºå›¾ç‰‡å¯èƒ½æœ‰è´¨é‡é—®é¢˜ï¼Œä¸‹æ¸¸å¯è¿›è¡Œè¿‡æ»¤ã€‚  "max_component_area": 30976

}

## ğŸ“š API Reference / API å‚è€ƒ```



### Main Processing Functions / ä¸»è¦å¤„ç†å‡½æ•°### CSV Format (Multiple Images)

```csv

#### Single Image / å•å›¾å¤„ç†image,SSI,GCP,SSD,STR,CSI,COI,LQI,angle,bbox,scale,quality_flag,skel_pixels,max_component_area

```pythonsample_1.png,0.82,0.91,0.74,0.68,0.59,0.45,0.71,0.3,"[40, 216, 40, 216]",1.0,True,425,30976

from libian_metrics.preprocess import preprocesssample_2.png,0.75,0.88,0.71,0.72,0.61,0.48,0.69,0.1,"[38, 218, 38, 218]",1.0,True,512,31500

from libian_metrics.metrics import compute_all_metrics```



# Preprocess image## Configuration Files

bin_img, skel, meta = preprocess(img_bgr)

### calibration.json

# Compute metricsGenerated from sample images using `calibrate_from_folder()`:

metrics = compute_all_metrics(bin_img, skel)

# Returns: {SSI, GCP, SSD, STR, CSI, COI, LQI}```json

```{

  "r_cap": 2.85,

#### Batch Processing / æ‰¹é‡å¤„ç†  "c_cap": 0.92,

```python  "angle_thresh_deg": 28.5,

from libian_metrics.batch_process import process_dataset_folder, print_results  "density_alpha": 0.6,

  "num_samples": 50,

# Process entire dataset  "r_values_percentiles": {

results = process_dataset_folder(    "min": 1.02,

    'data/my_dataset',    "p25": 1.15,

    calib=None,    "p50": 1.28,

    output_json='results/output.json'    "p75": 1.45,

)    "max": 2.98

  },

# Print formatted results  "cv_values_percentiles": {

print_results(results, detailed=True)    "min": 0.15,

```    "p25": 0.32,

    "p50": 0.58,

## ğŸ› Troubleshooting / æ•…éšœæ’é™¤    "p75": 0.78,

    "max": 1.08

**Issue**: "No images found in folder"  }

- **Solution**: Check that images are in the correct subdirectories and have supported extensions}

```

**Issue**: Metrics are all very low (< 0.1)

- **Solution**: Image may be upside down or inverted. Check `quality_flag` is True## Notes



**Issue**: "Module not found" error- All six metrics are **single-image computable** and independent

- **Solution**: Install package with `pip install -e .` from the project root- Suitable for comparative analysis across time periods (e.g., Chu vs. Han)

- Preprocessing is deterministic (no random components)

**é—®é¢˜**: "æ‰¾ä¸åˆ°å›¾ç‰‡"- All metrics are normalized to [0, 1] range

- **è§£å†³**: æ£€æŸ¥å›¾ç‰‡æ˜¯å¦åœ¨æ­£ç¡®çš„å­æ–‡ä»¶å¤¹ä¸­ï¼Œä¸”æ–‡ä»¶æ‰©å±•åæ”¯æŒ- Output is JSON by default, CSV append mode for batch processing



**é—®é¢˜**: æ‰€æœ‰æŒ‡æ ‡éƒ½å¾ˆä½ (< 0.1)## Citation

- **è§£å†³**: å›¾ç‰‡å¯èƒ½é¢ å€’æˆ–åè‰²ã€‚æ£€æŸ¥ `quality_flag` æ˜¯å¦ä¸º True

If you use LiBian Metrics in academic work, please cite:

**é—®é¢˜**: "æ¨¡å—æœªæ‰¾åˆ°"é”™è¯¯

- **è§£å†³**: ä»é¡¹ç›®æ ¹ç›®å½•ç”¨ `pip install -e .` å®‰è£…åŒ…```

LiBian Metrics: A Python Toolkit for Ancient Character Glyph Quality Assessment

## ğŸ“„ License / è®¸å¯è¯```



MIT License - See LICENSE file for details## License



## ğŸ¤ Contributing / è´¡çŒ®[Specify your license here]



Contributions are welcome! Please feel free to submit issues or pull requests.## Author



## ğŸ“ Support / æ”¯æŒLiShift Team



For bugs, feature requests, or questions, please open an issue on GitHub.## Support



---For issues, feature requests, or contributions, please contact the development team.


**Version**: 1.0.0  
**Last Updated**: 2024-01-06  
**Author**: LiShift Team

---

## File Structure / æ–‡ä»¶ç»“æ„

```
LiShift/
â”œâ”€â”€ libian_metrics/              # Main package / ä¸»åŒ…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __main__.py             # CLI entry point / CLIå…¥å£
â”‚   â”œâ”€â”€ cli.py                  # Command-line interface / å‘½ä»¤è¡Œç•Œé¢
â”‚   â”œâ”€â”€ preprocess.py           # Image preprocessing / å›¾åƒé¢„å¤„ç†
â”‚   â”œâ”€â”€ metrics.py              # Metric computation / æŒ‡æ ‡è®¡ç®—
â”‚   â”œâ”€â”€ batch_process.py        # Batch processing / æ‰¹é‡å¤„ç†
â”‚   â”œâ”€â”€ calibrate.py            # Calibration utilities / æ ¡å‡†å·¥å…·
â”‚   â”œâ”€â”€ io_utils.py             # I/O utilities / è¾“å…¥è¾“å‡ºå·¥å…·
â”‚   â”œâ”€â”€ skeleton.py             # Skeleton utilities / éª¨æ¶å·¥å…·
â”‚   â””â”€â”€ visualize.py            # Visualization / å¯è§†åŒ–
â”œâ”€â”€ data/                        # Data folder (put your datasets here) / æ•°æ®æ–‡ä»¶å¤¹
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ results/                     # Results folder (output saved here) / ç»“æœæ–‡ä»¶å¤¹
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ setup.py
â””â”€â”€ README.md                    # This file / æœ¬æ–‡ä»¶
```

## Quick Reference / å¿«é€Ÿå‚è€ƒ

```bash
# Process single image / å¤„ç†å•å¼ å›¾ç‰‡
python -m libian_metrics --image char.jpg

# Batch process / æ‰¹é‡å¤„ç†
python -m libian_metrics --dataset data/my_dataset --detailed

# Save output to file / è¾“å‡ºåˆ°æ–‡ä»¶
python -m libian_metrics --dataset data/my_dataset --out results/output.json

# Use calibration file / ä½¿ç”¨æ ¡å‡†æ–‡ä»¶
python -m libian_metrics --dataset data/my_dataset --calib calibration.json

# Show help / æ˜¾ç¤ºå¸®åŠ©
python -m libian_metrics --help
```

## ä½¿ç”¨æµç¨‹æ€»ç»“ / Workflow Summary

1. **å‡†å¤‡æ•°æ® / Prepare Data**
   ```bash
   mkdir -p data/my_dataset/{ç”²,ä¹™,ä¸™}
   # Copy images to character folders
   ```

2. **è¿è¡Œå¤„ç† / Run Processing**
   ```bash
   python -m libian_metrics --dataset data/my_dataset --out results/output.json --detailed
   ```

3. **æŸ¥çœ‹ç»“æœ / View Results**
   ```bash
   cat results/output.json
   ```

4. **åˆ†æç»“æœ / Analyze Results**
   - Check `summary.average_LQI` for overall quality
   - Compare metrics across characters
   - Identify outliers or quality issues


